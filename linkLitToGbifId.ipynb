{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13744c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os  # Import os module for directory management\n",
    "import zipfile\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API endpoint and initial parameters\n",
    "api_url = \"https://api.gbif.org/v1/literature/search\"\n",
    "params = {\n",
    "    \"contentType\": \"literature\",\n",
    "    \"literatureType\": [\"JOURNAL\", \"WORKING_PAPER\",\"BOOK\",\"BOOK_SECTION\"],\n",
    "    \"relevance\": \"GBIF_CITED\",\n",
    "    \"peerReview\": \"true\",\n",
    "    \"limit\": 10,\n",
    "    \"offset\": 0  # Start from the beginning\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9264f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get data from the API\n",
    "def fetch_data(params):\n",
    "    response = requests.get(api_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all entries and filter those with content in gbifDownloadKey\n",
    "def extract_filtered_entries():\n",
    "    all_entries = []\n",
    "    params['offset'] = 0  # Ensure offset starts at 0\n",
    "    \n",
    "    # Fetch initial data to determine total number of results\n",
    "    initial_data = fetch_data(params)\n",
    "    if not initial_data or 'count' not in initial_data:\n",
    "        print(\"Failed to fetch initial data or count not available.\")\n",
    "        return []\n",
    "    \n",
    "    total_results = initial_data['count']\n",
    "    print(f\"Total results to fetch: {total_results}\")\n",
    "    \n",
    "    with tqdm(total=total_results, desc=\"Fetching entries\") as pbar:\n",
    "        while True:\n",
    "            data = fetch_data(params)\n",
    "            if data and 'results' in data:\n",
    "                # Filter entries that have content in gbifDownloadKey\n",
    "                filtered_entries = [entry for entry in data['results'] if entry.get('gbifDownloadKey')]\n",
    "                all_entries.extend(filtered_entries)\n",
    "                pbar.update(len(data['results']))\n",
    "                if len(data['results']) < params['limit']:\n",
    "                    # No more data to fetch\n",
    "                    break\n",
    "                else:\n",
    "                    # Move to the next page\n",
    "                    params['offset'] += params['limit']\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    return all_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and filter entries\n",
    "filtered_entries = extract_filtered_entries()\n",
    "\n",
    "# Optionally, save the data to a file\n",
    "with open('filtered_gbif_entries.json', 'w') as f:\n",
    "    json.dump(filtered_entries, f, indent=2)\n",
    "\n",
    "# Print the number of filtered entries fetched\n",
    "print(f\"Total filtered entries fetched: {len(filtered_entries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65821d7d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Summary:\n",
    "- **Increase Field Size Limit**: The script sets the field size limit for CSV processing to 1,000,000 characters to handle large fields.\n",
    "- **Load and Save Processed DOIs**: Functions to load and save DOIs to track which entries have been processed.\n",
    "- **Download and Process Data**: The main function to download, unzip, process, and filter data, ensuring only preserved specimens are kept, and appending results to an output file on the D drive.\n",
    "- **Directory Checks**: Ensures necessary directories exist before writing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3066394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the CSV field size limit to the maximum value\n",
    "max_int = sys.maxsize\n",
    "while True:\n",
    "    # Decrease the max size until the csv.field_size_limit() function works\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int / 10)\n",
    "\n",
    "# Function to load processed DOIs from skip file\n",
    "def load_processed_dois(skip_file):\n",
    "    print(f\"Loading processed DOIs from {skip_file}\")\n",
    "    if os.path.exists(skip_file):\n",
    "        with open(skip_file, 'r', encoding='utf-8') as file:\n",
    "            return set(line.strip() for line in file)\n",
    "    return set()\n",
    "\n",
    "# Function to save a DOI to the skip file\n",
    "def save_processed_doi(skip_file, doi):\n",
    "    print(f\"Saving DOI {doi} to {skip_file}\")\n",
    "    with open(skip_file, 'a', encoding='utf-8') as file:\n",
    "        file.write(doi + '\\n')\n",
    "\n",
    "# Function to load downloaded keys from a file\n",
    "def load_downloaded_keys(downloaded_keys_file):\n",
    "    print(f\"Loading downloaded keys from {downloaded_keys_file}\")\n",
    "    if os.path.exists(downloaded_keys_file):\n",
    "        with open(downloaded_keys_file, 'r', encoding='utf-8') as file:\n",
    "            return set(line.strip() for line in file)\n",
    "    return set()\n",
    "\n",
    "# Function to save a downloaded key to a file\n",
    "def save_downloaded_key(downloaded_keys_file, key):\n",
    "    print(f\"Saving downloaded key {key} to {downloaded_keys_file}\")\n",
    "    with open(downloaded_keys_file, 'a', encoding='utf-8') as file:\n",
    "        file.write(key + '\\n')\n",
    "\n",
    "# Function to download, unzip, process data using gbifDownloadKey, and delete zip files and extracted contents\n",
    "def download_and_process_gbif_data(filtered_entries, skip_file, downloaded_keys_file):\n",
    "    base_url = \"https://api.gbif.org/v1/occurrence/download/request/\"\n",
    "    download_dir = \"D:/gbif_downloads\"  # Change to D drive\n",
    "    error_log = \"D:/gbif_errors/error_log.txt\"  # Change to D drive and use a subdirectory\n",
    "    output_file = \"D:/gbif_outputs/output_data.csv\"  # Change to D drive and use a subdirectory\n",
    "    \n",
    "    # Ensure the directories exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    if not os.path.exists(os.path.dirname(error_log)):\n",
    "        os.makedirs(os.path.dirname(error_log))\n",
    "    if not os.path.exists(os.path.dirname(output_file)):\n",
    "        os.makedirs(os.path.dirname(output_file))\n",
    "    if not os.path.exists(os.path.dirname(skip_file)):\n",
    "        os.makedirs(os.path.dirname(skip_file))\n",
    "    if not os.path.exists(os.path.dirname(downloaded_keys_file)):\n",
    "        os.makedirs(os.path.dirname(downloaded_keys_file))\n",
    "    \n",
    "    # Load processed DOIs\n",
    "    processed_dois = load_processed_dois(skip_file)\n",
    "    print(f\"Loaded {len(processed_dois)} processed DOIs\")\n",
    "\n",
    "    # Load downloaded keys\n",
    "    downloaded_keys = load_downloaded_keys(downloaded_keys_file)\n",
    "    print(f\"Loaded {len(downloaded_keys)} downloaded keys\")\n",
    "    \n",
    "    # Determine if we need to write the header\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    \n",
    "    # Open the output CSV file in append mode\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['gbifID', 'year', 'countryCode', 'gbifDownloadKey', 'doi']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header if the file did not exist before\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "            print(f\"Wrote header to {output_file}\")\n",
    "        \n",
    "        with open(error_log, 'w') as error_file:\n",
    "            for entry in tqdm(filtered_entries, desc=\"Downloading and processing GBIF data\"):\n",
    "                try:\n",
    "                    identifiers = entry.get('identifiers', {})\n",
    "                    doi = identifiers.get('doi', '')\n",
    "                    if doi in processed_dois:\n",
    "                        print(f\"Skipping already processed DOI: {doi}\")\n",
    "                        continue\n",
    "                    \n",
    "                    key = entry.get('gbifDownloadKey', [])[0]\n",
    "                    if key in downloaded_keys:\n",
    "                        print(f\"Skipping already downloaded key: {key}\")\n",
    "                        continue\n",
    "                    \n",
    "                    file_path = os.path.join(download_dir, f\"{key}.zip\")\n",
    "                    \n",
    "                    # Check if file already exists\n",
    "                    if os.path.exists(file_path):\n",
    "                        print(f\"File {file_path} already exists. Skipping download.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Download the zip file\n",
    "                    download_url = f\"{base_url}{key}.zip\"\n",
    "                    print(f\"Downloading {download_url}\")\n",
    "                    response = requests.get(download_url, stream=True)\n",
    "                    if response.status_code == 200:\n",
    "                        with open(file_path, 'wb') as file:\n",
    "                            for chunk in response.iter_content(chunk_size=1024):\n",
    "                                file.write(chunk)\n",
    "                        print(f\"Downloaded {file_path}\")\n",
    "                        # Save the downloaded key\n",
    "                        save_downloaded_key(downloaded_keys_file, key)\n",
    "                        downloaded_keys.add(key)\n",
    "                    elif response.status_code == 404:\n",
    "                        error_message = f\"Failed to download data for key {key}: 404 Not Found\"\n",
    "                        error_file.write(error_message + '\\n')\n",
    "                        print(error_message)\n",
    "                        continue\n",
    "                    else:\n",
    "                        error_message = f\"Failed to download data for key {key}: {response.status_code}\"\n",
    "                        error_file.write(error_message + '\\n')\n",
    "                        print(error_message)\n",
    "                        continue\n",
    "                \n",
    "                    # Unzip the downloaded file and extract required information\n",
    "                    try:\n",
    "                        print(f\"Unzipping {file_path}\")\n",
    "                        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                            zip_ref.extractall(download_dir)\n",
    "                            extracted_files = zip_ref.namelist()\n",
    "                            \n",
    "                            # Check for occurrence.txt (Darwin Core archive) or single CSV file\n",
    "                            occurrence_file_path = None\n",
    "                            if 'occurrence.txt' in extracted_files:\n",
    "                                occurrence_file_path = os.path.join(download_dir, 'occurrence.txt')\n",
    "                            else:\n",
    "                                csv_file_name = f\"{key}.csv\"\n",
    "                                if csv_file_name in extracted_files:\n",
    "                                    occurrence_file_path = os.path.join(download_dir, csv_file_name)\n",
    "                            \n",
    "                            if occurrence_file_path:\n",
    "                                print(f\"Processing {occurrence_file_path}\")\n",
    "                                with open(occurrence_file_path, newline='', encoding='utf-8') as occurrence_file:\n",
    "                                    reader = csv.DictReader(occurrence_file, delimiter='\\t')\n",
    "                                    # Normalize column names to lower case\n",
    "                                    reader.fieldnames = [field.lower() for field in reader.fieldnames]\n",
    "                                    # Print the column names for debugging\n",
    "                                    print(f\"Column names: {reader.fieldnames}\")\n",
    "                                    for row in reader:\n",
    "                                        # Check for the presence of necessary columns\n",
    "                                        if 'gbifid' not in row or 'year' not in row or 'countrycode' not in row:\n",
    "                                            raise KeyError(\"One or more expected columns are missing.\")\n",
    "                                        # Check for both 'basisofrecord' in a case-insensitive manner\n",
    "                                        basis_of_record = row.get('basisofrecord', '').lower()\n",
    "                                        if basis_of_record == 'preserved_specimen'.lower():\n",
    "                                            writer.writerow({\n",
    "                                                'gbifID': row['gbifid'],\n",
    "                                                'year': row['year'],\n",
    "                                                'countryCode': row['countrycode'],\n",
    "                                                'gbifDownloadKey': key,\n",
    "                                                'doi': doi\n",
    "                                            })\n",
    "                                print(f\"Processed {occurrence_file_path}\")\n",
    "                                # Ensure the file is closed before deleting it\n",
    "                                del reader\n",
    "                                os.remove(occurrence_file_path)\n",
    "                                print(f\"Deleted extracted file {occurrence_file_path}\")\n",
    "                            \n",
    "                            # Delete all other extracted files\n",
    "                            for extracted_file in extracted_files:\n",
    "                                extracted_file_path = os.path.join(download_dir, extracted_file)\n",
    "                                if os.path.exists(extracted_file_path):\n",
    "                                    os.remove(extracted_file_path)\n",
    "                                    print(f\"Deleted file {extracted_file_path}\")\n",
    "                        \n",
    "                        # Ensure the zip file is closed before deleting it\n",
    "                        del zip_ref\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Deleted {file_path}\")\n",
    "                        \n",
    "                        # Save the DOI to the skip file\n",
    "                        save_processed_doi(skip_file, doi)\n",
    "                        print(f\"Saved DOI {doi} to skip file\")\n",
    "                    except zipfile.BadZipFile:\n",
    "                        error_message = f\"Bad zip file {file_path}\"\n",
    "                        error_file.write(error_message + '\\n')\n",
    "                        print(error_message)\n",
    "                    except KeyError as e:\n",
    "                        error_message = f\"Missing expected column in file {file_path}: {str(e)}\"\n",
    "                        error_file.write(error_message + '\\n')\n",
    "                        print(error_message)\n",
    "                    except Exception as e:\n",
    "                        error_message = f\"Failed to process file {file_path}: {str(e)}\"\n",
    "                        error_file.write(error_message + '\\n')\n",
    "                        print(error_message)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    error_message = f\"Request error for key {key}: {str(e)}\"\n",
    "                    error_file.write(error_message + '\\n')\n",
    "                    print(error_message)\n",
    "                except Exception as e:\n",
    "                    error_message = f\"Unexpected error for key {key}: {str(e)}\"\n",
    "                    error_file.write(error_message + '\\n')\n",
    "                    print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e39fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the filtered entries and specify the skip file and downloaded keys file\n",
    "skip_file = \"D:/gbif_skip_files/processed_dois.txt\"  # Change to D drive and use a subdirectory\n",
    "downloaded_keys_file = \"D:/gbif_skip_files/downloaded_keys.txt\"  # Change to D drive and use a subdirectory\n",
    "download_and_process_gbif_data(filtered_entries, skip_file, downloaded_keys_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28899d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
