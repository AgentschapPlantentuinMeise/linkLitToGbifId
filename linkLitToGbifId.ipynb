{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13744c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os  # Import os module for directory management\n",
    "import zipfile\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53fb823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API endpoint and initial parameters\n",
    "api_url = \"https://api.gbif.org/v1/literature/search\"\n",
    "params = {\n",
    "    \"contentType\": \"literature\",\n",
    "    \"literatureType\": [\"journal\", \"working_paper\"],\n",
    "    \"relevance\": \"GBIF_USED\",\n",
    "    \"peerReview\": \"true\",\n",
    "    \"limit\": 10,\n",
    "    \"offset\": 0  # Start from the beginning\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9264f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get data from the API\n",
    "def fetch_data(params):\n",
    "    response = requests.get(api_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f60f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all entries and filter those with content in gbifDownloadKey\n",
    "def extract_filtered_entries():\n",
    "    all_entries = []\n",
    "    params['offset'] = 0  # Ensure offset starts at 0\n",
    "    \n",
    "    # Fetch initial data to determine total number of results\n",
    "    initial_data = fetch_data(params)\n",
    "    if not initial_data or 'count' not in initial_data:\n",
    "        print(\"Failed to fetch initial data or count not available.\")\n",
    "        return []\n",
    "    \n",
    "    total_results = initial_data['count']\n",
    "    print(f\"Total results to fetch: {total_results}\")\n",
    "    \n",
    "    with tqdm(total=total_results, desc=\"Fetching entries\") as pbar:\n",
    "        while True:\n",
    "            data = fetch_data(params)\n",
    "            if data and 'results' in data:\n",
    "                # Filter entries that have content in gbifDownloadKey\n",
    "                filtered_entries = [entry for entry in data['results'] if entry.get('gbifDownloadKey')]\n",
    "                all_entries.extend(filtered_entries)\n",
    "                pbar.update(len(data['results']))\n",
    "                if len(data['results']) < params['limit']:\n",
    "                    # No more data to fetch\n",
    "                    break\n",
    "                else:\n",
    "                    # Move to the next page\n",
    "                    params['offset'] += params['limit']\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    return all_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4206f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results to fetch: 10585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching entries: 100%|██████████| 10585/10585 [04:38<00:00, 38.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filtered entries fetched: 3572\n"
     ]
    }
   ],
   "source": [
    "# Extract and filter entries\n",
    "filtered_entries = extract_filtered_entries()\n",
    "\n",
    "# Optionally, save the data to a file\n",
    "with open('filtered_gbif_entries.json', 'w') as f:\n",
    "    json.dump(filtered_entries, f, indent=2)\n",
    "\n",
    "# Print the number of filtered entries fetched\n",
    "print(f\"Total filtered entries fetched: {len(filtered_entries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65821d7d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Summary:\n",
    "- **Increase Field Size Limit**: The script sets the field size limit for CSV processing to 1,000,000 characters to handle large fields.\n",
    "- **Load and Save Processed DOIs**: Functions to load and save DOIs to track which entries have been processed.\n",
    "- **Download and Process Data**: The main function to download, unzip, process, and filter data, ensuring only preserved specimens are kept, and appending results to an output file on the D drive.\n",
    "- **Directory Checks**: Ensures necessary directories exist before writing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3066394",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Increase the CSV field size limit to a large value\n",
    "csv.field_size_limit(10**6)\n",
    "\n",
    "# Function to load processed DOIs from skip file\n",
    "def load_processed_dois(skip_file):\n",
    "    print(f\"Loading processed DOIs from {skip_file}\")\n",
    "    if os.path.exists(skip_file):\n",
    "        with open(skip_file, 'r', encoding='utf-8') as file:\n",
    "            return set(line.strip() for line in file)\n",
    "    return set()\n",
    "\n",
    "# Function to save a DOI to the skip file\n",
    "def save_processed_doi(skip_file, doi):\n",
    "    print(f\"Saving DOI {doi} to {skip_file}\")\n",
    "    with open(skip_file, 'a', encoding='utf-8') as file:\n",
    "        file.write(doi + '\\n')\n",
    "\n",
    "# Function to load downloaded keys from a file\n",
    "def load_downloaded_keys(downloaded_keys_file):\n",
    "    print(f\"Loading downloaded keys from {downloaded_keys_file}\")\n",
    "    if os.path.exists(downloaded_keys_file):\n",
    "        with open(downloaded_keys_file, 'r', encoding='utf-8') as file:\n",
    "            return set(line.strip() for line in file)\n",
    "    return set()\n",
    "\n",
    "# Function to save a downloaded key to a file\n",
    "def save_downloaded_key(downloaded_keys_file, key):\n",
    "    print(f\"Saving downloaded key {key} to {downloaded_keys_file}\")\n",
    "    with open(downloaded_keys_file, 'a', encoding='utf-8') as file:\n",
    "        file.write(key + '\\n')\n",
    "\n",
    "# Function to download, unzip, process data using gbifDownloadKey, and delete zip files and extracted contents\n",
    "def download_and_process_gbif_data(filtered_entries, skip_file, downloaded_keys_file):\n",
    "    base_url = \"https://api.gbif.org/v1/occurrence/download/request/\"\n",
    "    download_dir = \"D:/gbif_downloads\"  # Change to D drive\n",
    "    error_log = \"D:/gbif_errors/error_log.txt\"  # Change to D drive and use a subdirectory\n",
    "    output_file = \"D:/gbif_outputs/output_data.csv\"  # Change to D drive and use a subdirectory\n",
    "    \n",
    "    # Ensure the directories exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    if not os.path.exists(os.path.dirname(error_log)):\n",
    "        os.makedirs(os.path.dirname(error_log))\n",
    "    if not os.path.exists(os.path.dirname(output_file)):\n",
    "        os.makedirs(os.path.dirname(output_file))\n",
    "    if not os.path.exists(os.path.dirname(skip_file)):\n",
    "        os.makedirs(os.path.dirname(skip_file))\n",
    "    if not os.path.exists(os.path.dirname(downloaded_keys_file)):\n",
    "        os.makedirs(os.path.dirname(downloaded_keys_file))\n",
    "    \n",
    "    # Load processed DOIs\n",
    "    processed_dois = load_processed_dois(skip_file)\n",
    "    print(f\"Loaded {len(processed_dois)} processed DOIs\")\n",
    "\n",
    "    # Load downloaded keys\n",
    "    downloaded_keys = load_downloaded_keys(downloaded_keys_file)\n",
    "    print(f\"Loaded {len(downloaded_keys)} downloaded keys\")\n",
    "    \n",
    "    # Determine if we need to write the header\n",
    "    write_header = not os.path.exists(output_file)\n",
    "    \n",
    "    # Open the output CSV file in append mode\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['gbifID', 'year', 'countryCode', 'gbifDownloadKey', 'doi']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header if the file did not exist before\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "            print(f\"Wrote header to {output_file}\")\n",
    "        \n",
    "        with open(error_log, 'w') as error_file:\n",
    "            for entry in tqdm(filtered_entries, desc=\"Downloading and processing GBIF data\"):\n",
    "                try:\n",
    "                    identifiers = entry.get('identifiers', {})\n",
    "                    doi = identifiers.get('doi', '')\n",
    "                    if doi in processed_dois:\n",
    "                        print(f\"Skipping already processed DOI: {doi}\")\n",
    "                        continue\n",
    "                    \n",
    "                    key = entry.get('gbifDownloadKey', [])[0]\n",
    "                    if key in downloaded_keys:\n",
    "                        print(f\"Skipping already downloaded key: {key}\")\n",
    "                        continue\n",
    "                    \n",
    "                    file_path = os.path.join(download_dir, f\"{key}.zip\")\n",
    "                    \n",
    "                    # Check if file already exists\n",
    "                    if os.path.exists(file_path):\n",
    "                        print(f\"File {file_path} already exists. Skipping download.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Download the zip file\n",
    "                    download_url = f\"{base_url}{key}.zip\"\n",
    "                    print(f\"Downloading {download_url}\")\n",
    "                    response = requests.get(download_url, stream=True)\n",
    "                    if response.status_code == 200:\n",
    "                        with open(file_path, 'wb') as file:\n",
    "                            for chunk in response.iter_content(chunk_size=1024):\n",
    "                                file.write(chunk)\n",
    "                        print(f\"Downloaded {file_path}\")\n",
    "                        # Save the downloaded key\n",
    "                        save_downloaded_key(downloaded_keys_file, key)\n",
    "                        downloaded_keys.add(key)\n",
    "                    elif response.status_code == 404:\n",
    "                        error_message = f\"Failed to download data for key {key}: 404 Not Found\"\n",
    "                        error_file.write(error_message + '\\n')\n",
    "                        print(error_message)\n",
    "                        continue\n",
    "                    else:\n",
    "                        error_message = f\"Failed to download data for key {key}: {response.status_code}\"\n",
    "                        error_file.write(error_message + '\\n')\n",
    "                        print(error_message)\n",
    "                        continue\n",
    "                \n",
    "                    # Unzip the downloaded file and extract required information\n",
    "                    try:\n",
    "                        print(f\"Unzipping {file_path}\")\n",
    "                        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                            zip_ref.extractall(download_dir)\n",
    "                            extracted_files = zip_ref.namelist()\n",
    "                            print(f\"Extracted files: {extracted_files}\")\n",
    "                            \n",
    "                            # Check for occurrence.txt (Darwin Core archive) or single CSV file\n",
    "                            occurrence_file_path = None\n",
    "                            if 'occurrence.txt' in extracted_files:\n",
    "                                occurrence_file_path = os.path.join(download_dir, 'occurrence.txt')\n",
    "                            else:\n",
    "                                csv_file_name = f\"{key}.csv\"\n",
    "                                if csv_file_name in extracted_files:\n",
    "                                    occurrence_file_path = os.path.join(download_dir, csv_file_name)\n",
    "                            \n",
    "                            if occurrence_file_path:\n",
    "                                print(f\"Processing {occurrence_file_path}\")\n",
    "                                with open(occurrence_file_path, newline='', encoding='utf-8') as occurrence_file:\n",
    "                                    reader = csv.DictReader(occurrence_file, delimiter='\\t')\n",
    "                                    for row in reader:\n",
    "                                        # Check for both 'basisOfRecord' and 'basisofrecord' in a case-insensitive manner\n",
    "                                        basis_of_record = row.get('basisOfRecord', '').lower() if 'basisOfRecord' in row else row.get('basisofrecord', '').lower()\n",
    "                                        if basis_of_record == 'preserved_specimen'.lower():\n",
    "                                            writer.writerow({\n",
    "                                                'gbifID': row['gbifID'],\n",
    "                                                'year': row['year'],\n",
    "                                                'countryCode': row['countryCode'],\n",
    "                                                'gbifDownloadKey': key,\n",
    "                                                'doi': doi\n",
    "                                            })\n",
    "                                print(f\"Processed {occurrence_file_path}\")\n",
    "                                # Ensure the file is closed before deleting it\n",
    "                                del reader\n",
    "                                os.remove(occurrence_file_path)\n",
    "                                print(f\"Deleted extracted file {occurrence_file_path}\")\n",
    "                        \n",
    "                        # Ensure the zip file is closed before deleting it\n",
    "                        del zip_ref\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Deleted {file_path}\")\n",
    "                        \n",
    "                        # Save the DOI to the skip file\n",
    "                        save_processed_doi(skip_file, doi)\n",
    "                        print(f\"Saved DOI {doi} to skip file\")\n",
    "                    except zipfile.BadZipFile:\n",
    "                        error_message = f\"Bad zip file {file_path}\"\n",
    "                        error_file.write(error_message + '\\n')\n",
    "                        print(error_message)\n",
    "                    except Exception as e:\n",
    "                        error_message = f\"Failed to process file {file_path}: {str(e)}\"\n",
    "                        error_file.write(error_message + '\\n')\n",
    "                        print(error_message)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    error_message = f\"Request error for key {key}: {str(e)}\"\n",
    "                    error_file.write(error_message + '\\n')\n",
    "                    print(error_message)\n",
    "                except Exception as e:\n",
    "                    error_message = f\"Unexpected error for key {key}: {str(e)}\"\n",
    "                    error_file.write(error_message + '\\n')\n",
    "                    print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e39fbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed DOIs from D:/gbif_skip_files/processed_dois.txt\n",
      "Loaded 186 processed DOIs\n",
      "Loading downloaded keys from D:/gbif_skip_files/downloaded_keys.txt\n",
      "Loaded 152 downloaded keys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading and processing GBIF data:   0%|          | 0/3572 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already processed DOI: 10.15666/aeer/2202_18851902\n",
      "Skipping already processed DOI: 10.1002/ece3.11230\n",
      "Skipping already processed DOI: 10.1038/s41598-024-59947-y\n",
      "Skipping already downloaded key: 0019000-220831081235567\n",
      "Skipping already processed DOI: 10.1016/j.tfp.2024.100559\n",
      "Skipping already processed DOI: 10.1007/s10530-024-03313-6\n",
      "Skipping already processed DOI: 10.32383/appdr/185727\n",
      "Skipping already processed DOI: 10.3390/fishes9040148\n",
      "Skipping already processed DOI: 10.1007/s10681-024-03317-2\n",
      "Skipping already processed DOI: 10.1007/s10750-024-05554-x\n",
      "Skipping already processed DOI: 10.1016/j.ecoinf.2024.102604\n",
      "Skipping already processed DOI: 10.1093/jee/toae013\n",
      "Skipping already processed DOI: 10.3897/zookeys.1196.116144\n",
      "Skipping already processed DOI: 10.37828/em.2024.72.20\n",
      "Skipping already processed DOI: 10.7494/geom.2024.18.3.45\n",
      "Skipping already processed DOI: 10.1007/s10113-024-02222-7\n",
      "Skipping already processed DOI: 10.13057/biodiv/d250328\n",
      "Skipping already processed DOI: 10.1002/ajb2.16322\n",
      "Skipping already processed DOI: 10.1111/eea.13451\n",
      "Skipping already processed DOI: 10.1111/oik.10217\n",
      "Skipping already processed DOI: 10.1016/j.scitotenv.2024.172519\n",
      "Downloading https://api.gbif.org/v1/occurrence/download/request/0014654-230224095556074.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading and processing GBIF data:   1%|          | 22/3572 [00:00<00:57, 61.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download data for key 0014654-230224095556074: 404 Not Found\n",
      "Skipping already processed DOI: 10.1002/tax.13173\n",
      "Skipping already downloaded key: 0202277-220831081235567\n",
      "Skipping already processed DOI: 10.1002/ecs2.4837\n",
      "Skipping already processed DOI: 10.1002/ps.8128\n",
      "Skipping already processed DOI: 10.15560/20.2.536\n",
      "Skipping already processed DOI: 10.1111/eff.12784\n",
      "Skipping already downloaded key: 0001340-160118175350007\n",
      "Skipping already processed DOI: 10.1111/gcb.17282\n",
      "Skipping already processed DOI: 10.1093/botlinnean/boae019\n",
      "Skipping already processed DOI: 10.1007/s10340-024-01767-0\n",
      "Skipping already processed DOI: 10.1038/s41558-024-01966-8\n",
      "Skipping already processed DOI: 10.1002/ecs2.4830\n",
      "Skipping already processed DOI: 10.1111/geb.13847\n",
      "Skipping already processed DOI: 10.26577/eb.2024.v98.i1.010\n",
      "Skipping already processed DOI: 10.1038/s41467-024-46818-3\n",
      "Skipping already processed DOI: 10.3389/fevo.2024.1346795\n",
      "Skipping already processed DOI: 10.3390/d16040223\n",
      "Skipping already downloaded key: 0329580-210914110416597\n",
      "Skipping already processed DOI: 10.11646/phytotaxa.639.1.1\n",
      "Skipping already processed DOI: 10.3897/bdj.12.e118854\n",
      "Skipping already processed DOI: 10.1111/jbi.14840\n",
      "Skipping already processed DOI: 10.21829/abm131.2024.2287\n",
      "Skipping already processed DOI: 10.1007/s13744-024-01148-3\n",
      "Skipping already downloaded key: 0235288-200613084148143\n",
      "Skipping already processed DOI: 10.7717/peerj.17178\n",
      "Skipping already processed DOI: 10.1111/njb.04250\n",
      "Skipping already processed DOI: 10.1002/ajb2.16308\n",
      "Skipping already processed DOI: 10.3897/zookeys.1197.114679\n",
      "Skipping already processed DOI: 10.1016/j.sajb.2024.03.022\n",
      "Skipping already processed DOI: 10.1007/s11056-024-10040-2\n",
      "Skipping already processed DOI: 10.1080/17550874.2024.2328832\n",
      "Skipping already processed DOI: 10.1007/s10531-024-02831-y\n",
      "Skipping already processed DOI: 10.7717/peerj.17210\n",
      "Skipping already processed DOI: 10.1093/jee/toae060\n",
      "Skipping already processed DOI: 10.3390/conservation4020011\n",
      "Skipping already processed DOI: 10.17129/botsci.3364\n",
      "Skipping already processed DOI: 10.1111/jbi.14843\n",
      "Skipping already processed DOI: 10.1111/jbi.14837\n",
      "Skipping already processed DOI: 10.1002/tax.13170\n",
      "Skipping already downloaded key: 0294933-200613084148143\n",
      "Skipping already processed DOI: 10.1071/bt23104\n",
      "Skipping already processed DOI: 10.1007/s11101-024-09934-y\n",
      "Skipping already downloaded key: 0065480-210914110416597\n",
      "Skipping already processed DOI: 10.59763/mam.aeq.v6i.71\n",
      "Skipping already processed DOI: 10.1111/ddi.13839\n",
      "Skipping already processed DOI: 10.1002/ece3.11200\n",
      "Skipping already processed DOI: 10.1007/s10750-024-05501-w\n",
      "Skipping already processed DOI: 10.1007/s00338-024-02490-z\n",
      "Skipping already processed DOI: 10.1038/s41598-024-57590-1\n",
      "Skipping already processed DOI: 10.1590/1809-4392202301392\n",
      "Skipping already processed DOI: 10.1093/botlinnean/boad065\n",
      "Skipping already processed DOI: 10.1080/17451000.2024.2309562\n",
      "Skipping already processed DOI: 10.3390/d16040205\n",
      "Skipping already processed DOI: 10.17129/botsci.3389\n",
      "Skipping already processed DOI: 10.7717/peerj.17131\n",
      "Skipping already processed DOI: 10.1007/s10336-024-02168-x\n",
      "Skipping already processed DOI: 10.1016/j.egg.2024.100240\n",
      "Skipping already processed DOI: 10.1134/s003294522470005x\n",
      "Skipping already processed DOI: 10.1016/j.jenvman.2024.120757\n",
      "Skipping already processed DOI: 10.1134/s1062359023605554\n",
      "Skipping already processed DOI: 10.1134/s0032945224700073\n",
      "Skipping already processed DOI: 10.1093/biosci/biae022\n",
      "Skipping already processed DOI: 10.1111/gcb.17262\n",
      "Skipping already processed DOI: 10.1016/j.ecss.2024.108735\n",
      "Skipping already downloaded key: 0018093-180508205500799\n",
      "Skipping already processed DOI: 10.11609/jott.8574.16.3.24967-24972\n",
      "Skipping already processed DOI: 10.3897/herpetozoa.37.e117370\n",
      "Skipping already processed DOI: 10.1371/journal.pone.0295102\n",
      "Downloading https://api.gbif.org/v1/occurrence/download/request/0294639-200613084148143.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading and processing GBIF data:   3%|▎         | 91/3572 [00:00<00:23, 146.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download data for key 0294639-200613084148143: 404 Not Found\n",
      "Skipping already processed DOI: 10.1051/alr/2024002\n",
      "Skipping already processed DOI: 10.1007/s10530-024-03283-9\n",
      "Skipping already downloaded key: 0046560-210914110416597\n",
      "Skipping already processed DOI: 10.1016/j.revpalbo.2024.105096\n",
      "Skipping already processed DOI: 10.3390/biology13030198\n",
      "Skipping already processed DOI: 10.1093/jxb/erae126\n",
      "Skipping already processed DOI: 10.3897/bdj.12.e120670\n",
      "Skipping already processed DOI: 10.1093/botlinnean/boae016\n",
      "Skipping already processed DOI: 10.1007/s10661-024-12543-z\n",
      "Skipping already processed DOI: 10.1111/ecog.06697\n",
      "Skipping already processed DOI: 10.1016/j.vetpar.2024.110172\n",
      "Skipping already processed DOI: 10.22271/letters.2024.v4.i1b.85\n",
      "Skipping already processed DOI: 10.1111/njb.04266\n",
      "Skipping already processed DOI: 10.3390/su16051929\n",
      "Skipping already processed DOI: 10.1007/s11258-024-01408-7\n",
      "Skipping already processed DOI: 10.1002/ece3.11132\n",
      "Skipping already processed DOI: 10.1002/ece3.11097\n",
      "Skipping already processed DOI: 10.1016/j.ecochg.2024.100084\n",
      "Skipping already processed DOI: 10.1016/j.scitotenv.2024.171664\n",
      "Skipping already processed DOI: 10.1111/jfb.15724\n",
      "Skipping already processed DOI: 10.1016/j.gecco.2024.e02903\n",
      "Skipping already processed DOI: 10.1111/jbi.14834\n",
      "Skipping already processed DOI: 10.11646/zootaxa.5424.4.8\n",
      "Skipping already processed DOI: 10.18473/lepi.78i1.a8\n",
      "Skipping already processed DOI: 10.3390/insects15030195\n",
      "Skipping already processed DOI: 10.1093/evolut/qpae034\n",
      "Skipping already processed DOI: 10.1016/j.aquaculture.2024.740815\n",
      "Skipping already processed DOI: 10.1098/rsos.230603\n",
      "Skipping already processed DOI: 10.1111/jbi.14828\n",
      "Skipping already processed DOI: 10.1111/1365-2435.14543\n",
      "Skipping already processed DOI: 10.1111/icad.12731\n",
      "Skipping already processed DOI: 10.1111/zsc.12656\n",
      "Skipping already processed DOI: 10.1111/ecog.06996\n",
      "Skipping already processed DOI: 10.3391/mbi.2024.15.1.09\n",
      "Skipping already processed DOI: 10.1186/s42408-024-00259-x\n",
      "Skipping already processed DOI: 10.2903/j.efsa.2024.8498\n",
      "Skipping already processed DOI: 10.1016/j.quaint.2024.03.004\n",
      "Skipping already processed DOI: 10.3390/biology13030177\n",
      "Skipping already processed DOI: 10.1016/s2666-5247(23)00396-8\n",
      "Skipping already processed DOI: 10.28947/hrmo.2023.24.2.748\n",
      "Skipping already processed DOI: 10.1016/j.rvsc.2024.105206\n",
      "Skipping already processed DOI: 10.1111/gcb.17232\n",
      "Skipping already processed DOI: 10.1111/1440-1703.12458\n",
      "Skipping already downloaded key: 0216203-200613084148143\n",
      "Skipping already processed DOI: 10.15560/20.2.279\n",
      "Skipping already processed DOI: 10.5751/jfo-00432-950109\n",
      "Skipping already processed DOI: 10.22543/0090-0222.2462\n",
      "Skipping already processed DOI: 10.1659/mrd.2023.00017\n",
      "Skipping already processed DOI: 10.1007/s00267-024-01955-9\n",
      "Skipping already processed DOI: 10.1038/s41559-024-02350-7\n",
      "Skipping already processed DOI: 10.1038/s41558-024-01941-3\n",
      "Skipping already processed DOI: 10.1002/ece3.11092\n",
      "Skipping already processed DOI: 10.1111/plb.13635\n",
      "Skipping already processed DOI: 10.1111/ecog.07085\n",
      "Skipping already processed DOI: 10.1007/s10344-024-01780-9\n",
      "Skipping already processed DOI: 10.1038/s41598-024-56158-3\n",
      "Skipping already processed DOI: 10.1002/2688-8319.12310\n",
      "Skipping already processed DOI: 10.1002/ece3.11066\n",
      "Skipping already processed DOI: 10.1016/j.jaridenv.2024.105139\n",
      "Skipping already processed DOI: 10.1016/j.ecss.2024.108699\n",
      "Skipping already processed DOI: 10.1093/jee/toae018\n",
      "Skipping already processed DOI: 10.1177/09596836241231446\n",
      "Skipping already processed DOI: 10.1038/s41477-024-01649-4\n",
      "Skipping already downloaded key: 0162648-220831081235567\n",
      "Skipping already processed DOI: 10.1111/ddi.13826\n",
      "Skipping already processed DOI: 10.1002/ece3.10901\n",
      "Skipping already processed DOI: 10.1111/1365-2664.14616\n",
      "Skipping already processed DOI: 10.1111/jbi.14826\n",
      "Skipping already processed DOI: 10.1111/icad.12730\n",
      "Skipping already processed DOI: 10.1007/s10530-024-03268-8\n",
      "Skipping already processed DOI: 10.1007/s10841-024-00564-5\n",
      "Skipping already processed DOI: 10.7717/peerj.17029\n",
      "Skipping already processed DOI: 10.1111/mec.17304\n",
      "Skipping already processed DOI: 10.1002/ajb2.16299\n",
      "Skipping already processed DOI: 10.1007/s10530-024-03279-5\n",
      "Skipping already processed DOI: 10.24189/ncr.2024.005\n",
      "Skipping already processed DOI: 10.22201/fc.25942158e.2024.1.860\n",
      "Downloading https://api.gbif.org/v1/occurrence/download/request/0000129-150523225239109.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading and processing GBIF data:   5%|▍         | 169/3572 [00:01<00:18, 185.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download data for key 0000129-150523225239109: 404 Not Found\n",
      "Skipping already processed DOI: 10.1038/s41598-024-54735-0\n",
      "Skipping already downloaded key: 0014439-240202131308920\n",
      "Skipping already processed DOI: 10.5751/es-14793-290121\n",
      "Skipping already processed DOI: 10.1111/eff.12771\n",
      "Skipping already processed DOI: 10.1007/s10530-024-03270-0\n",
      "Skipping already processed DOI: 10.1016/j.marpolbul.2024.116162\n",
      "Skipping already processed DOI: 10.12976/jib/2024.46.1.2\n",
      "Skipping already processed DOI: 10.12976/jib/2024.46.1.1\n",
      "Skipping already downloaded key: 0178283-220831081235567\n",
      "Skipping already downloaded key: 0008475-230530130749713\n",
      "Skipping already downloaded key: 0015937-231120084113126\n",
      "Skipping already processed DOI: 10.1016/j.gecco.2024.e02861\n",
      "Skipping already processed DOI: 10.5597/lajam00321\n",
      "Skipping already downloaded key: 0259755-220831081235567\n",
      "Skipping already processed DOI: 10.1111/gcb.17205\n",
      "Skipping already processed DOI: 10.1111/ele.14391\n",
      "Skipping already processed DOI: 10.1111/jbi.14820\n",
      "File D:/gbif_downloads\\0000740-180508205500799.zip already exists. Skipping download.\n",
      "Skipping already processed DOI: 10.3390/life14030283\n",
      "Skipping already processed DOI: 10.5852/ejt.2024.921.2433\n",
      "Skipping already processed DOI: 10.1016/j.baae.2024.02.008\n",
      "Skipping already processed DOI: 10.3157/061.150.0103\n",
      "Skipping already processed DOI: 10.3390/jof10030166\n",
      "Skipping already processed DOI: 10.25225/jvb.23084\n",
      "Skipping already processed DOI: 10.1007/s00442-024-05516-z\n",
      "Skipping already processed DOI: 10.1007/s10841-024-00551-w\n",
      "Skipping already processed DOI: 10.1080/09524622.2024.2309361\n",
      "Skipping already processed DOI: 10.1111/nph.19601\n",
      "Skipping already processed DOI: 10.1111/nph.19580\n",
      "Skipping already downloaded key: 0329580-210914110416597\n",
      "Skipping already processed DOI: 10.1038/s41598-024-54699-1\n",
      "Skipping already downloaded key: 0417875-210914110416597\n",
      "Skipping already processed DOI: 10.1007/s12526-024-01404-0\n",
      "Skipping already processed DOI: 10.1016/j.geosus.2024.01.009\n",
      "Skipping already processed DOI: 10.1016/j.biocon.2024.110503\n",
      "Skipping already processed DOI: 10.1093/treephys/tpae019\n",
      "Skipping already processed DOI: 10.12933/therya_notes-24-142\n",
      "Skipping already downloaded key: 0286915-200613084148143\n",
      "Skipping already processed DOI: 10.1007/s00484-024-02634-4\n",
      "Skipping already processed DOI: 10.1007/s44274-024-00029-1\n",
      "File D:/gbif_downloads\\0047555-180508205500799.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0224656-230224095556074.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0237268-220831081235567.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0060514-210914110416597.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0073030-220831081235567.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0013634-230530130749713.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0236416-200613084148143.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0001688-240229165702484.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0026674-231120084113126.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0218647-200613084148143.zip already exists. Skipping download.\n",
      "File D:/gbif_downloads\\0354752-210914110416597.zip already exists. Skipping download.\n",
      "Downloading https://api.gbif.org/v1/occurrence/download/request/0072734-210914110416597.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading and processing GBIF data:   6%|▌         | 221/3572 [00:01<00:18, 179.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded D:/gbif_downloads\\0072734-210914110416597.zip\n",
      "Saving downloaded key 0072734-210914110416597 to D:/gbif_skip_files/downloaded_keys.txt\n",
      "Unzipping D:/gbif_downloads\\0072734-210914110416597.zip\n",
      "Extracted files: ['0072734-210914110416597.csv']\n",
      "Processing D:/gbif_downloads\\0072734-210914110416597.csv\n",
      "Processed D:/gbif_downloads\\0072734-210914110416597.csv\n",
      "Deleted extracted file D:/gbif_downloads\\0072734-210914110416597.csv\n",
      "Deleted D:/gbif_downloads\\0072734-210914110416597.zip\n",
      "Saving DOI 10.1007/s42965-024-00329-w to D:/gbif_skip_files/processed_dois.txt\n",
      "Saved DOI 10.1007/s42965-024-00329-w to skip file\n",
      "Downloading https://api.gbif.org/v1/occurrence/download/request/0016949-231002084531237.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading and processing GBIF data:   6%|▌         | 221/3572 [00:13<00:18, 179.27it/s]"
     ]
    }
   ],
   "source": [
    "# Call the function with the filtered entries and specify the skip file and downloaded keys file\n",
    "skip_file = \"D:/gbif_skip_files/processed_dois.txt\"  # Change to D drive and use a subdirectory\n",
    "downloaded_keys_file = \"D:/gbif_skip_files/downloaded_keys.txt\"  # Change to D drive and use a subdirectory\n",
    "download_and_process_gbif_data(filtered_entries, skip_file, downloaded_keys_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28899d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
